<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="In this article, we will try to model a Leaky Spiking Neuron (LIF) using digital hardware: registers, memories, adders and so on."><title>Spiking neurons: a digital hardware implementation</title><link rel=canonical href=https://open-neuromorphic.org/p/spiking-neurons-a-digital-hardware-implementation/><link rel=stylesheet href=/scss/style.min.8191399262444ab68b72a18c97392f5349be20a1615d77445be51e974c144cff.css><meta property="og:title" content="Spiking neurons: a digital hardware implementation"><meta property="og:description" content="In this article, we will try to model a Leaky Spiking Neuron (LIF) using digital hardware: registers, memories, adders and so on."><meta property="og:url" content="https://open-neuromorphic.org/p/spiking-neurons-a-digital-hardware-implementation/"><meta property="og:site_name" content="Open Neuromorphic"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="hardware"><meta property="article:tag" content="digital"><meta property="article:tag" content="spiking"><meta property="article:tag" content="snn"><meta property="article:tag" content="rtl"><meta property="article:tag" content="verilog"><meta property="article:tag" content="AI"><meta property="article:tag" content="machine learning"><meta property="article:published_time" content="2023-01-02T00:00:00+00:00"><meta property="article:modified_time" content="2023-01-02T00:00:00+00:00"><meta property="og:image" content="https://open-neuromorphic.org/p/spiking-neurons-a-digital-hardware-implementation/loihi.png"><meta name=twitter:title content="Spiking neurons: a digital hardware implementation"><meta name=twitter:description content="In this article, we will try to model a Leaky Spiking Neuron (LIF) using digital hardware: registers, memories, adders and so on."><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://open-neuromorphic.org/p/spiking-neurons-a-digital-hardware-implementation/loihi.png"><link rel="shortcut icon" href=/img/ONM-logo.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/ONM-logo.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Open Neuromorphic</a></h1><h2 class=site-description>Organization that aims at providing one place to reference all relevant open-source project in the neuromorphic research domain.</h2></div></header><ol class=social-menu><li><a href=https://discord.gg/C9bzWgNmqk target=_blank title=Discord rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-discord" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#2c3e50" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="9" cy="12" r="1"/><circle cx="15" cy="12" r="1"/><path d="M7.5 7.5c3.5-1 5.5-1 9 0"/><path d="M7 16.5c3.5 1 6.5 1 10 0"/><path d="M15.5 17c0 1 1.5 3 2 3 1.5.0 2.833-1.667 3.5-3 .667-1.667.5-5.833-1.5-11.5-1.457-1.015-3-1.34-4.5-1.5l-1 2.5"/><path d="M8.5 17c0 1-1.356 3-1.832 3-1.429.0-2.698-1.667-3.333-3-.635-1.667-.476-5.833 1.428-11.5C6.151 4.485 7.545 4.16 9 4l1 2.5"/></svg></a></li><li><a href=https://github.com/open-neuromorphic target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.linkedin.com/groups/9267873 target=_blank title=LinkedIn rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-linkedin" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#2c3e50" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="4" y="4" width="16" height="16" rx="2"/><line x1="8" y1="11" x2="8" y2="16"/><line x1="8" y1="8" x2="8" y2="8.01"/><line x1="12" y1="16" x2="12" y2="11"/><path d="M16 16v-3a2 2 0 00-4 0"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/events/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-event" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="4" y="5" width="16" height="16" rx="2"/><line x1="16" y1="3" x2="16" y2="7"/><line x1="8" y1="3" x2="8" y2="7"/><line x1="4" y1="11" x2="20" y2="11"/><rect x="8" y="15" width="2" height="2"/></svg><span>Events</span></a></li><li><a href=/opportunities/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-bulb" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 12h1m8-9v1m8 8h1M5.6 5.6l.7.7m12.1-.7-.7.7"/><path d="M9 16a5 5 0 116 0 3.5 3.5.0 00-1 3 2 2 0 01-4 0 3.5 3.5.0 00-1-3"/><line x1="9.7" y1="17" x2="14.3" y2="17"/></svg><span>Opportunities</span></a></li><li><a href=/resources/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-code" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><polyline points="7 8 3 12 7 16"/><polyline points="17 8 21 12 17 16"/><line x1="14" y1="4" x2="10" y2="20"/></svg><span>Resources</span></a></li><li><a href=/team/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-users" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="9" cy="7" r="4"/><path d="M3 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/><path d="M16 3.13a4 4 0 010 7.75"/><path d="M21 21v-2a4 4 0 00-3-3.85"/></svg><span>Team</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-open-source" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M12 3a9 9 0 013.618 17.243l-2.193-5.602a3 3 0 10-2.849.0l-2.193 5.603A9 9 0 0112 3z"/></svg><span>About</span></a></li><li><a href=/getting-involved/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-code" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><polyline points="7 8 3 12 7 16"/><polyline points="17 8 21 12 17 16"/><line x1="14" y1="4" x2="10" y2="20"/></svg><span>Getting involved</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#spiking-neurons>Spiking neurons</a></li><li><a href=#discretising-the-model>Discretising the model</a></li><li><a href=#storage-and-addressing-neurons-states>Storage and addressing neurons states</a></li><li><a href=#spikes-accumulation>Spikes accumulation</a></li><li><a href=#excitatory-and-inhibitory-neurons>Excitatory and inhibitory neurons</a></li><li><a href=#leakage>Leakage</a></li><li><a href=#spike-mechanism>Spike mechanism</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#acknowledgements>Acknowledgements</a></li><li><a href=#credits>Credits</a></li><li><a href=#authors>Authors</a></li><li><a href=#bibliography>Bibliography</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/spiking-neurons-a-digital-hardware-implementation/><img src=/p/spiking-neurons-a-digital-hardware-implementation/loihi_hubbeea76b711d6bc9f1399bce5b467dbf_1011331_800x0_resize_box_3.png srcset="/p/spiking-neurons-a-digital-hardware-implementation/loihi_hubbeea76b711d6bc9f1399bce5b467dbf_1011331_800x0_resize_box_3.png 800w, /p/spiking-neurons-a-digital-hardware-implementation/loihi_hubbeea76b711d6bc9f1399bce5b467dbf_1011331_1600x0_resize_box_3.png 1600w" width=800 height=778 loading=lazy alt="Featured image of post Spiking neurons: a digital hardware implementation"></a></div><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/p/spiking-neurons-a-digital-hardware-implementation/>Spiking neurons: a digital hardware implementation</a></h2><h3 class=article-subtitle>In this article, we will try to model a Leaky Spiking Neuron (LIF) using digital hardware: registers, memories, adders and so on.</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Jan 02, 2023</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>12 minute read</time></div></footer></div></header><section class=article-content><h2 id=spiking-neurons>Spiking neurons</h2><p>In this article, we will try to model a layer of Leaky Integrate and Fire (LIF) spiking neurons using digital hardware: registers, memories, adders and so on. To do so, we will consider a single output neuron connected to multiple input neurons from a previous layer.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/neurons-connected.png width=2061 height=744 srcset="/p/spiking-neurons-a-digital-hardware-implementation/neurons-connected_hufde41aca3783fa5bf23a53f4b1be2dc3_93109_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/neurons-connected_hufde41aca3783fa5bf23a53f4b1be2dc3_93109_1024x0_resize_box_3.png 1024w" loading=lazy alt="Multiple pre-synaptic neurons connected to a post-synaptic one." class=gallery-image data-flex-grow=277 data-flex-basis=664px></p><p>In a Spiking Neural Network (SNN), neurons communicate by means of <strong>spikes</strong>: these activation voltages are then converted to currents through the <strong>synapses</strong>, charging the <strong>membrane potential</strong> of the destination neuron. In the following, the destination neuron is denoted as <strong>post-synaptic</strong> neuron, with the index $i$, while the input neuron under consideration is denoted as <strong>pre-synaptic</strong> neuron, with the index $j$.</p><p>We denote the input spike train incoming from the pre-synaptic neuron with $\sigma_{j}(t)$:
$$ \sigma_{j}(t) = \sum_{k} \delta(t-t_{k}) $$
where $t_{k}$ are the spike timestamps of the spike train $\sigma_{j}(t)$.</p><p>The <strong>synapse</strong> connecting the pre-synaptic neuron with the post-synaptic neuron is denoted with $w_{ij}$. All the incoming spike trains are then <strong>integrated</strong> by the post-synaptic neuron membrane; the integration function can be modeled by a <strong>first-order low-pass filter</strong>, denoted with $\alpha_{i}(t)$:
$$ \alpha_{i}(t) = \frac{1}{\tau_{u_{i}}} e^{-\frac{t}{\tau_{u_{i}}}}$$
The spike train incoming from the pre-synaptic neuron, hence, is convolved with the membrane function; in real neurons, this corresponds to the <strong>input currents</strong> coming from the pre-synaptic neurons that <strong>charge</strong> the post-synaptic neuron membrane potential, $v_{i}(t)$. The sum of the currents in input to the post-synaptic neuron is denoted with $u_{i}(t)$ and modeled through the following equation:
$$ u_{i}(t) = \sum_{j \neq i}{w_{ij} \cdot (\alpha_{v} \ast \sigma_{j})(t)} $$
Each pre-synaptic neuron contributes with a current (spike train multiplied by the $w_{ij}$ synapse) and these sum up at the input of the post-synaptic neuron. Given the membrane potential of the destination neuron, denoted with $v_{i}(t)$, the differential equation describing its evolution through time is the following:
$$ \frac{\partial}{\partial t} v_{i}(t) = -\frac{1}{\tau_{v}} v_{i}(t) + u_{i}(t)$$
In addition to the input currents, we have the <strong>neuron leakage</strong>, $\frac{1}{\tau_{v}} v_{i}(t)$, modeled through a <strong>leakage coefficient</strong> $\frac{1}{\tau_{v}}$ that multiplies the membrane potential.</p><h2 id=discretising-the-model>Discretising the model</h2><p>Such a differential equation cannot be solved directly using discrete arithmetic, as it would be processed on digital hardware; hence, we need to <strong>discretise</strong> the equation. This discretisation leads to the following result:
$$ v_{i}[t] = \beta \cdot v_{i}[t-1] + (1 - \beta) \cdot u_{i}[t] - \theta \cdot S_{i}[t] $$
where $\beta$ is the <strong>decay coefficient</strong> associated to the leakage. We embed $(1-\beta)$ in the input current $u_{i}[t]$, by merging it with the synapse weights as a scaling factor; in this way, the input current $u_{i}[t]$ is <strong>normalised</strong> regardless of the decay constant $\tau_{v}$ value.</p><p>Notice that the <strong>membrane reset</strong> mechanism has been added: when a neuron <strong>spikes</strong>, its membrane potential goes back to the rest potential (usually equal to zero), and this is modeled by <strong>subtracting the threshold</strong> $\theta$ from $v_{i}(t)$ when an output spike occurs. The output spike is modeled through a function $S_{i}[t]$:
$$ S_{i}[t] = 1 ~\text{if}~ v_{i}[t] \gt \theta ~\text{else}~ 0 $$
This is equal to 1 at spike time (i.e. if at timestamp $t$ the membrane potential $v_{i}[t]$ is larger than the threshold $\theta$) and 0 elsewhere.</p><p>The input current is given by:
$$ u_{i}[t] = \sum_{j \neq i}{w_{ij} \cdot S_{j}[t]} $$<br>Notice that since $S_{i}[t]$ is either 0 or 1, the input current $u_{i}[t]$ is equal to the <strong>sum of the synapses weights</strong> of the pre-synaptic neurons that spike at timestamp $t$.</p><h2 id=storage-and-addressing-neurons-states>Storage and addressing neurons states</h2><p>Let us define the layer <strong>fan-in</strong>, i.e. how many pre-synaptic neurons are connected in input to each post-synaptic neuron in the layer; we denote this number with $N$. Then, we set the total number of neurons in our layer to $M$.</p><p>How do we describe a neuron in hardware? First of all, we need to list some basic information associated to each post-synaptic neuron:</p><ul><li>its <strong>membrane potential</strong> $v_{i}[t]$.</li><li>the <strong>weights associated with the synapses</strong>, $w_{ij}$; since each post-synaptic neuron is connected in input to $N$ neurons, these synapses can be grouped in an $N$-entries vector $W_{i}$.</li></ul><p>Since there are $M$ neurons in the layer, we need an $M$-entries vector, denoted with $V[t]$, to store the membrane potentials values evaluated at timestamp $t$; this vector is associated with a <strong>memory array</strong> in the hardware architecture.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/membrane-potentials.png width=1561 height=794 srcset="/p/spiking-neurons-a-digital-hardware-implementation/membrane-potentials_hu5bbfd51006a32eb425545f88180c0c97_62942_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/membrane-potentials_hu5bbfd51006a32eb425545f88180c0c97_62942_1024x0_resize_box_3.png 1024w" loading=lazy alt="The membrane potentials memory." class=gallery-image data-flex-grow=196 data-flex-basis=471px></p><p>An <strong>address</strong> is associated to each neuron, which can be thought as the $i$ index in the $V[t]$ vector; to obtain $v_{i}[t]$, the post-synaptic neuron address is used to index the membrane potentials memory $V[t]$.</p><p>We are able to store and retrieve a post-synaptic neuron membrane potential using a memory; now, we would like to <strong>charge it with the pre-synaptic neurons currents</strong> in order to emulate the behaviour of a neuron membrane; to do that, we need to get the corresponding input synapses $W_{i}$, <strong>multiply</strong> these by the spikes of the associated pre-synaptic neurons, sum them up and, then, accumulate these in the post-synaptic neuron membrane.</p><p>Let us start from a single input pre-synaptic neuron:
$$ u_{ij}[t] = w_{ij} \cdot S_{j}[t] $$
We know that $S_{j}[t]$ is either 1 or 0; hence, we have either $u_{ij}[t] = w_{ij}$ or $u_{ij}[t] = 0$; this means that the synapse weight is <strong>either added or not</strong> to the total current $u_{i}[t]$; hence, the weight $w_{ij}$ is read from memory <strong>only if the corresponding pre-synaptic neuron spikes!</strong> Given a layer of $M$ neurons, each of which is connected in input to $N$ synapses, we can think of grouping the $M \cdot N$ weights in a <strong>matrix</strong>, which can be associated with another memory array, denoted with $W$.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/synapses-weights.png width=2231 height=874 srcset="/p/spiking-neurons-a-digital-hardware-implementation/synapses-weights_hu67c5768f8825891815b89681ae240b1d_83094_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/synapses-weights_hu67c5768f8825891815b89681ae240b1d_83094_1024x0_resize_box_3.png 1024w" loading=lazy alt="The synapses weights memory." class=gallery-image data-flex-grow=255 data-flex-basis=612px></p><p>This memory is addressed with the pre-synaptic neuron and the post-synaptic neuron indices to retrieve the weight $w_{ij}$, which automatically corresponds to the $u_{ij}[t]$ current being accumulated in the post-synaptic neuron membrane when the pre-synaptic neuron spikes at timestamp $t$.</p><h2 id=spikes-accumulation>Spikes accumulation</h2><p>Let us implement neural functionalities using the data structures defined for a neuron (i.e. membrane potential and synapses), starting with the <strong>membrane potential charging</strong> of a post-synaptic neuron. When the pre-synaptic neuron spikes, its synapse weight $w_{ij}$ gets extracted from the synapse memory $W$ and multiplied by the spike; since the spike is a <strong>digital bit</strong> equal to 1, this is equivalent to <strong>using $w_{ij}$ itself as input current</strong> for the post-synaptic neuron; to add this current to $v_{i}[t]$, we need to use an arithmetic circuit called <strong>adder</strong>!</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/accumulation.png width=1612 height=1016 srcset="/p/spiking-neurons-a-digital-hardware-implementation/accumulation_hu0fc8f88676c742058e81209d5a8437fb_76462_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/accumulation_hu0fc8f88676c742058e81209d5a8437fb_76462_1024x0_resize_box_3.png 1024w" loading=lazy alt="The spikes accumulation circuit." class=gallery-image data-flex-grow=158 data-flex-basis=380px></p><p>The membrane potential $v_{i}[t]$ is read from the potentials memory $V[t]$ and added to the corresponding synapse current $w_{ij}$; the result is the membrane potential of the next time step, $v_{i}[t+1]$, that is stored in the <strong>register</strong> put on the adder output; this value is written back to the $V[t]$ memory in the next clock cycle. The register storing the adder output is denoted as <strong>membrane register</strong>.</p><p>To <strong>prevent multiple read-write cycles</strong> due to multiple spiking pre-synaptic neurons, one can think of adding a <strong>loop</strong> to the membrane register in order to <strong>accumulate all the currents</strong> of the pre-synaptic neurons that are spiking at timestep $t$ and writing the final value $v_{i}[t+1]$ back to memory <strong>only once</strong>. The corresponding circuit is shown in the following.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/accumulation-loop.png width=1799 height=1013 srcset="/p/spiking-neurons-a-digital-hardware-implementation/accumulation-loop_hu868ee57fbbd85ed207d780f73293926a_83987_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/accumulation-loop_hu868ee57fbbd85ed207d780f73293926a_83987_1024x0_resize_box_3.png 1024w" loading=lazy alt="Adding a loop register to accumulate multiple spikes before the write-back to memory." class=gallery-image data-flex-grow=177 data-flex-basis=426px></p><p>A <strong>multiplexer</strong> is placed on one side of the adder; in this way:</p><ul><li>the first weight $w_{i0}$ to be accumulated is added to the $v_{i}[t]$ read from memory and saved to the membrane register:
$$ v_{i}[t+1] = v_{i}[t] + w_{i0} $$</li><li>the successive weights are added to the membrane register content, so that all the currents are accumulated before writing $v_{i}[t+1]$ back to memory; using a non rigorous notation, this can be translated to the following equation:
$$ v_{i}[t+1] = v_{i}[t+1] + w_{ij},~ 0 \lt j \leq N $$</li></ul><h2 id=excitatory-and-inhibitory-neurons>Excitatory and inhibitory neurons</h2><p>Our post-synaptic neuron is able to accumulate spikes in its membrane; however, input spikes do not always result in membrane potential charging! In fact, a pre-synaptic neuron can be <strong>excitatory</strong> (i.e. it <strong>charges</strong> the post-synaptic neuron membrane) or <strong>inhibitory</strong> (i.e. it <strong>discharges</strong> the post-synaptic neuron membrane); in the digital circuit, this phenomenon corresponds to <strong>adding</strong> or <strong>subtracting</strong>, respectively, the synapse weight $w_{ij}$ to or from $v_{i}[t]$; this functionality can be added to the architecture by placing an adder capable of performing <strong>both additions and subtractions</strong>, choosing among these with a control signal generated by an <strong>FSM (Finite State Machine)</strong>, which is a sequential digital circuit that evolves through a series of states depending on its inputs and, consequently, generates controls signals for the rest of the circuit.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/inhibitory.png width=1612 height=1016 srcset="/p/spiking-neurons-a-digital-hardware-implementation/inhibitory_huca9ce09602971950892899452f33315f_93376_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/inhibitory_huca9ce09602971950892899452f33315f_93376_1024x0_resize_box_3.png 1024w" loading=lazy alt="Control circuit for choosing between excitatory and inhibitory stimula." class=gallery-image data-flex-grow=158 data-flex-basis=380px></p><p>This FSM, given the operation to be executed on the post-synaptic neuron, chooses if the adder has to add or subtract the synapse current.</p><p>However, is this design efficient in terms of resources employed? It has to be reminded that inhibitory and excitatory neurons are chosen at <strong>chip programming time</strong>; this means that <strong>the neuron type does not change during the chip operation</strong> (however, with the solution we are about to propose, it would not be a problem to change the neuron type on-the-fly); hence, we can <strong>embed this information</strong> in the neuron description by <strong>adding a bit to the synapses weights memory row</strong> that, depending on its value, denotes that neuron as excitatory or inhibitory.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/synapse-encoding.png width=400 height=146 srcset="/p/spiking-neurons-a-digital-hardware-implementation/synapse-encoding_hu15da4478632c6b64a6b83607aeaa5423_8608_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/synapse-encoding_hu15da4478632c6b64a6b83607aeaa5423_8608_1024x0_resize_box_3.png 1024w" loading=lazy alt="Synapses weight storage in memory." class=gallery-image data-flex-grow=273 data-flex-basis=657px></p><p>Suppose that, given a pre-synaptic neuron, all its $M$ output synapses are stored in a memory row of $n$ bits words, where $n$ is the number of bits to which the synapse weight is quantized. At the end of the memory row $j$, we add a bit denoted with $e_{j}$ that identifies the neuron type and that is read together with the weights from the same memory row: if the pre-synaptic neuron $j$ is <strong>excitatory</strong>, $e_{j}=1$ and the weight is <strong>added</strong>; if it is <strong>inhibitory</strong>, $e_{j}=0$ and the weight is <strong>subtracted</strong>; in this way, <strong>the $e_{j}$ field of the synapse can drive the adder directly</strong>.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/modified-adder.png width=1612 height=1016 srcset="/p/spiking-neurons-a-digital-hardware-implementation/modified-adder_huafc096ac915ea43ada89c8d177c22b8a_83317_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/modified-adder_huafc096ac915ea43ada89c8d177c22b8a_83317_1024x0_resize_box_3.png 1024w" loading=lazy alt="Using the neuron type bit to drive the adder." class=gallery-image data-flex-grow=158 data-flex-basis=380px></p><h2 id=leakage>Leakage</h2><p>Let us introduce the characteristic feature of the LIF neuron: the <strong>leakage</strong>! We shall choose a (constant) leakage factor $\beta$ and multiply it by $v_{i}[t]$ to obtain $v_{i}[t+1]$, which is <strong>lower</strong> than $v_{i}[t]$ since some current has leaked from the membrane, and we model this through $\beta$:
$$ v_{i}[t+1] = \beta \cdot v_{i}[t] $$
However, multiplication is an <strong>expensive</strong> operation in hardware; furthermore, the leakage factor is <strong>smaller than one</strong>, so we would need to perform a <strong>fixed-point multiplication</strong> or, even worse, a <strong>division</strong>! How can we solve this problem?</p><p>If we choose $\beta$ as a power of $\frac{1}{2}$, such as $2^{-n}$, the multiplication becomes <strong>equivalent to a $n$-positions right shift</strong>! A really <strong>hardware-friendly</strong> operation!</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/leak.png width=1612 height=1294 srcset="/p/spiking-neurons-a-digital-hardware-implementation/leak_hu6efb26cc282f1e71f25be47c1b1e2a9f_119349_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/leak_hu6efb26cc282f1e71f25be47c1b1e2a9f_119349_1024x0_resize_box_3.png 1024w" loading=lazy alt="Leakage circuit." class=gallery-image data-flex-grow=124 data-flex-basis=298px></p><p>In this circuit, an $n$-positions righ-shift block, denoted with the symbol <code>>></code>, is placed on one of the adder inputs to obtain $\beta \cdot v_{i}[t]$ from $v_{i}[t]$. A <strong>multiplexer</strong> is introduced to choose among the synapse weight $w_{ij}$ and the leakage contribution $\beta \cdot v_{i}[t]$ as input to the adder.</p><p>Notice that <strong>the leakage has to be always subtracted</strong> from the membrane potential; hence, we cannot use $e_{j}$ directly to control the adder but we must modify the circuit so that a subtraction is performed during a leakage operation, regardless of the value of $e_{j}$. A possible solution is to use a signal from the FSM and a <strong>logic AND gate</strong> to force the adder control signal to 0 during a leakage operation.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/subtract-leak.png width=1612 height=1294 srcset="/p/spiking-neurons-a-digital-hardware-implementation/subtract-leak_hud2de163dc45acbdca811dfc3794ca9d9_120661_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/subtract-leak_hud2de163dc45acbdca811dfc3794ca9d9_120661_1024x0_resize_box_3.png 1024w" loading=lazy alt="Simplified leakage circuit." class=gallery-image data-flex-grow=124 data-flex-basis=298px></p><p>Denoting with <code>adder_ctrl</code> the signal which controls the adder and with <code>leak_op_n</code> the one provided by the FSM, and stating that:</p><ul><li>for <code>adder_ctrl=1</code>, the adder performs an addition, otherwise a subtraction.</li><li><code>leak_op_n=0</code> when a leakage operation has to performed.</li></ul><p><code>adder_ctrl</code> can be obtained as the logic AND operation of <code>leak_op_n</code> and $e_{j}$ so that, when <code>leak_op_n=0</code>, <code>adder_ctrl=0</code> regardless of the value of $e_{j}$ and a subtraction is performed by the adder.</p><h2 id=spike-mechanism>Spike mechanism</h2><p>Our neuron needs to spike! If this is encoded as a single digital bit, given the spiking threshold $\theta$, we <strong>compare $v_{i}[t]$ to $\theta$</strong> and generate a logic 1 in output <strong>when the membrane potential is larger than the threshold</strong>. This can be implemented using a <strong>comparator</strong> circuit.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/spike.png width=1612 height=1635 srcset="/p/spiking-neurons-a-digital-hardware-implementation/spike_hu96016051d2ee2ee0e53c45c191e43005_138579_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/spike_hu96016051d2ee2ee0e53c45c191e43005_138579_1024x0_resize_box_3.png 1024w" loading=lazy alt="Spike circuit." class=gallery-image data-flex-grow=98 data-flex-basis=236px></p><p>The output of the comparator is used directly as <strong>spike bit</strong>.</p><p>The membrane has to be <strong>reset to a rest potential</strong> when the neuron spikes; hence, we need to <strong>subtract $\theta$ from $v_{i}[t]$ when the neuron fires</strong>. This can be done by driving the input multiplexer of the membrane register to <strong>provide $\theta$ in input to the adder</strong>, that performs a subtraction.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/reset.png width=1673 height=1635 srcset="/p/spiking-neurons-a-digital-hardware-implementation/reset_hu263ed7102fcea0fbf00e41d0963a3f26_147149_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/reset_hu263ed7102fcea0fbf00e41d0963a3f26_147149_1024x0_resize_box_3.png 1024w" loading=lazy alt="Membrane reset by threshold subtraction." class=gallery-image data-flex-grow=102 data-flex-basis=245px></p><p>This circuit can be simplified:</p><ul><li>by choosing $\theta = 2^m-1$, where $m$ is the <strong>bitwidth of the membrane register and the adder</strong>, having $v_{i}[t] \gt \theta$ is <strong>equivalent to having an overflow in the addition</strong>; hence, the comparison result is equal to the <strong>overflow flag</strong> of the adder, which can be <strong>provided directly in output as spike bit</strong>.</li><li>instead of subtracting $\theta$ from the membrane register, we can <strong>reset</strong> $v_{i}[t]$ to 0 when a spike occurs by forcing the membrane register content to 0 with a control signal; this is equivalent to using the oveflow flag of the adder as <strong>reset signal for the membrane register</strong>. This should not be done in an actual implementation: at least a <strong>register</strong> should be added on the reset signal of the membrane register to prevent glitches in the adder circuit from resetting it when it should not be.</li></ul><p>The resulting circuit is the following.</p><p><img src=/p/spiking-neurons-a-digital-hardware-implementation/smart-reset.png width=1756 height=1364 srcset="/p/spiking-neurons-a-digital-hardware-implementation/smart-reset_hud76884d98bd3d3ec19919eaa8e00ac8a_133275_480x0_resize_box_3.png 480w, /p/spiking-neurons-a-digital-hardware-implementation/smart-reset_hud76884d98bd3d3ec19919eaa8e00ac8a_133275_1024x0_resize_box_3.png 1024w" loading=lazy alt="Membrane reset by membrane potential zeroing." class=gallery-image data-flex-grow=128 data-flex-basis=308px></p><h2 id=conclusion>Conclusion</h2><p>Here we are, with a first prototype of our LIF layer digital circuit. In the next episode:</p><ul><li>we will make it actually work. Right now, this is a functional model, that needs some modifications to behave correctly as a spiking neurons layer.</li><li>we will implement it in Verilog.</li><li>we will simulate it using open source tools, such as <a class=link href=ihttps://www.veripool.org/verilator/>Verilator</a>.</li></ul><h2 id=acknowledgements>Acknowledgements</h2><p>I would like to thank <a class=link href=https://jasoneshraghian.com target=_blank rel=noopener>Jason Eshraghian</a>, <a class=link href=https://stevenabreu.com target=_blank rel=noopener>Steven Abreu</a> and <a class=link href=https://lenzgregor.com target=_blank rel=noopener>Gregor Lenz</a> for the valuable corrections and comments that made this article way better than the original draft!</p><h2 id=credits>Credits</h2><p>The cover image is the Loihi die, taken from <a class=link href=https://en.wikichip.org/wiki/intel/loihi target=_blank rel=noopener>WikiChip</a>.</p><h2 id=authors>Authors</h2><ul><li><a class=link href=https://fabrizio-ottati.dev target=_blank rel=noopener>Fabrizio Ottati</a> is a Ph.D. student in the HLS Laboratory of the Department of Electronics and Communications, Politecnico di Torino. His main interests are event-based cameras, digital hardware design and neuromorphic computing. He is one of the maintainers of two open source projects in the field of neuromorphic computing, <a class=link href=https://tonic.readthedocs.io target=_blank rel=noopener>Tonic</a> and <a class=link href=https://expelliarmus.readthedocs.io target=_blank rel=noopener>Expelliarmus</a>, and one of the founders of <a class=link href=https://open-neuromorphic.org target=_blank rel=noopener>Open Neuromorphic</a>.</li></ul><h2 id=bibliography>Bibliography</h2><ul><li><a class=link href=https://redwood.berkeley.edu/wp-content/uploads/2021/08/Davies2018.pdf target=_blank rel=noopener><em>Loihi: A Neuromorphic Manycore Processor with On-Chip Learning</em></a>, Mike Davies et al., 2018.</li><li><a class=link href=https://arxiv.org/abs/2109.12894 target=_blank rel=noopener><em>Training Spiking Neural Networks Using Lessons From Deep Learning</em></a>, Jason Eshraghian et al., 2022.</li><li><a class=link href=https://arxiv.org/abs/1804.07858 target=_blank rel=noopener><em>A 0.086-mm2 12.7-pJ/SOP 64k-Synapse 256-Neuron Online-Learning Digital Spiking Neuromorphic Processor in 28nm CMOS</em></a>, Charlotte Frenkel et al., 2019.</li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/hardware/>hardware</a>
<a href=/tags/digital/>digital</a>
<a href=/tags/spiking/>spiking</a>
<a href=/tags/snn/>snn</a>
<a href=/tags/rtl/>rtl</a>
<a href=/tags/verilog/>verilog</a>
<a href=/tags/ai/>AI</a>
<a href=/tags/machine-learning/>machine learning</a></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.css integrity="sha256-J+iAE0sgH8QSz9hpcDxXIftnj65JEZgNhGcgReTTK9s=" crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/katex.min.js integrity="sha256-InsNdER1b2xUewP+pKCUJpkhiqwHgqiPXDlIk7GzBu4=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.15.6/dist/contrib/auto-render.min.js integrity="sha256-y39Mpg7V3D4lhBX4x6O0bUqTV4pSrfgwEfGKfxkOdgI=" crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.querySelector(`.article-content`),{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})})</script></article><footer class=site-footer><section class=copyright>&copy;
2022 -
2023 Open Neuromorphic</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.16.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>