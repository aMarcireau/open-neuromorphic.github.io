<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="List of research articles related to digital hardware for neuromorphic applications."><title>Digital neuromophic hardware read list</title><link rel=canonical href=https://open-neuromorphic.org/p/digital-neuromophic-hardware-read-list/><link rel=stylesheet href=/scss/style.min.8191399262444ab68b72a18c97392f5349be20a1615d77445be51e974c144cff.css><meta property="og:title" content="Digital neuromophic hardware read list"><meta property="og:description" content="List of research articles related to digital hardware for neuromorphic applications."><meta property="og:url" content="https://open-neuromorphic.org/p/digital-neuromophic-hardware-read-list/"><meta property="og:site_name" content="Open Neuromorphic"><meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="research"><meta property="article:tag" content="hardware"><meta property="article:tag" content="digital"><meta property="article:tag" content="neuromorphic"><meta property="article:tag" content="snn"><meta property="article:tag" content="AI"><meta property="article:published_time" content="2023-01-11T00:00:00+00:00"><meta property="article:modified_time" content="2023-01-11T00:00:00+00:00"><meta property="og:image" content="https://open-neuromorphic.org/p/digital-neuromophic-hardware-read-list/frenkel-thesis.png"><meta name=twitter:title content="Digital neuromophic hardware read list"><meta name=twitter:description content="List of research articles related to digital hardware for neuromorphic applications."><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://open-neuromorphic.org/p/digital-neuromophic-hardware-read-list/frenkel-thesis.png"><link rel="shortcut icon" href=/img/ONM-logo.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/ONM-logo.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Open Neuromorphic</a></h1><h2 class=site-description>Organization that aims at providing one place to reference all relevant open-source project in the neuromorphic research domain.</h2></div></header><ol class=social-menu><li><a href=https://discord.gg/C9bzWgNmqk target=_blank title=Discord rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-discord" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#2c3e50" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="9" cy="12" r="1"/><circle cx="15" cy="12" r="1"/><path d="M7.5 7.5c3.5-1 5.5-1 9 0"/><path d="M7 16.5c3.5 1 6.5 1 10 0"/><path d="M15.5 17c0 1 1.5 3 2 3 1.5.0 2.833-1.667 3.5-3 .667-1.667.5-5.833-1.5-11.5-1.457-1.015-3-1.34-4.5-1.5l-1 2.5"/><path d="M8.5 17c0 1-1.356 3-1.832 3-1.429.0-2.698-1.667-3.333-3-.635-1.667-.476-5.833 1.428-11.5C6.151 4.485 7.545 4.16 9 4l1 2.5"/></svg></a></li><li><a href=https://github.com/open-neuromorphic target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.linkedin.com/groups/9267873 target=_blank title=LinkedIn rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-linkedin" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#2c3e50" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="4" y="4" width="16" height="16" rx="2"/><line x1="8" y1="11" x2="8" y2="16"/><line x1="8" y1="8" x2="8" y2="8.01"/><line x1="12" y1="16" x2="12" y2="11"/><path d="M16 16v-3a2 2 0 00-4 0"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li><a href=/events/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-event" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="4" y="5" width="16" height="16" rx="2"/><line x1="16" y1="3" x2="16" y2="7"/><line x1="8" y1="3" x2="8" y2="7"/><line x1="4" y1="11" x2="20" y2="11"/><rect x="8" y="15" width="2" height="2"/></svg><span>Events</span></a></li><li><a href=/opportunities/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-bulb" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 12h1m8-9v1m8 8h1M5.6 5.6l.7.7m12.1-.7-.7.7"/><path d="M9 16a5 5 0 116 0 3.5 3.5.0 00-1 3 2 2 0 01-4 0 3.5 3.5.0 00-1-3"/><line x1="9.7" y1="17" x2="14.3" y2="17"/></svg><span>Opportunities</span></a></li><li><a href=/resources/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-code" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><polyline points="7 8 3 12 7 16"/><polyline points="17 8 21 12 17 16"/><line x1="14" y1="4" x2="10" y2="20"/></svg><span>Resources</span></a></li><li><a href=/team/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-users" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="9" cy="7" r="4"/><path d="M3 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/><path d="M16 3.13a4 4 0 010 7.75"/><path d="M21 21v-2a4 4 0 00-3-3.85"/></svg><span>Team</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-open-source" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M12 3a9 9 0 013.618 17.243l-2.193-5.602a3 3 0 10-2.849.0l-2.193 5.603A9 9 0 0112 3z"/></svg><span>About</span></a></li><li><a href=/getting-involved/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-code" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><polyline points="7 8 3 12 7 16"/><polyline points="17 8 21 12 17 16"/><line x1="14" y1="4" x2="10" y2="20"/></svg><span>Getting involved</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#2015>2015</a><ol><li><a href=#truenorth-design-and-tool-flow-of-a-65-mw-1-million-neuron-programmable-neurosynaptic-chiphttpsredwoodberkeleyeduwp-contentuploads202108akopyan2015pdf-filipp-akopyan-et-al-2015><a href=https://redwood.berkeley.edu/wp-content/uploads/2021/08/Akopyan2015.pdf><em>TrueNorth: Design and Tool Flow of a 65 mW 1 Million Neuron Programmable Neurosynaptic Chip</em></a>, Filipp Akopyan et al., 2015</a></li></ol></li><li><a href=#2018>2018</a><ol><li><a href=#loihi-a-neuromorphic-manycore-processor-with-on-chip-learninghttpsredwoodberkeleyeduwp-contentuploads202108davies2018pdf-mike-davies-et-al-2018><a href=https://redwood.berkeley.edu/wp-content/uploads/2021/08/Davies2018.pdf><em>Loihi: A Neuromorphic Manycore Processor with On-Chip Learning</em></a>, Mike Davies et al., 2018</a></li></ol></li><li><a href=#2019>2019</a><ol><li><a href=#a-0086-mm2-127-pjsop-64k-synapse-256-neuron-online-learning-digital-spiking-neuromorphic-processor-in-28nm-cmoshttpsarxivorgabs180407858-charlotte-frenkel-et-al-2019><a href=https://arxiv.org/abs/1804.07858><em>A 0.086-mm2 12.7-pJ/SOP 64k-Synapse 256-Neuron Online-Learning Digital Spiking Neuromorphic Processor in 28nm CMOS</em></a>, Charlotte Frenkel et al., 2019</a></li><li><a href=#morphic-a-65-nm-738k-synapsemm2-quad-core-binary-weight-digital-neuromorphic-processor-with-stochastic-spike-driven-online-learninghttpsarxivorgabs190408513-charlotte-frenkel-et-al-2019><a href=https://arxiv.org/abs/1904.08513><em>MorphIC: A 65-nm 738k-Synapse/mm2 Quad-Core Binary-Weight Digital Neuromorphic Processor With Stochastic Spike-Driven Online Learning</em></a>, Charlotte Frenkel et al., 2019</a></li></ol></li><li><a href=#2020>2020</a><ol><li><a href=#always-on-sub-300-nw-event-driven-spiking-neural-network-based-on-spike-driven-clock-generation-and-clock--and-power-gating-for-an-ultra-low-power-intelligent-devicehttpsarxivorgabs200612314-dewei-wang-et-al-2020><a href=https://arxiv.org/abs/2006.12314><em>Always-On, Sub-300-nW, Event-Driven Spiking Neural Network based on Spike-Driven Clock-Generation and Clock- and Power-Gating for an Ultra-Low-Power Intelligent Device</em></a>, Dewei Wang et al., 2020</a></li></ol></li><li><a href=#2021>2021</a><ol><li><a href=#μbrain-an-event-driven-and-fully-synthesizable-architecture-for-spiking-neural-networkshttpswwwfrontiersinorgarticles103389fnins2021664208full-jan-stuijt-et-al-2021><a href=https://www.frontiersin.org/articles/10.3389/fnins.2021.664208/full><em>μBrain: An Event-Driven and Fully Synthesizable Architecture for Spiking Neural Networks</em></a>, Jan Stuijt et al., 2021</a></li></ol></li><li><a href=#2022>2022</a><ol><li><a href=#spiking-neural-network-integrated-circuits-a-review-of-trends-and-future-directionshttpsarxivorgabs220307006-arindam-basu-et-al-2022><a href=https://arxiv.org/abs/2203.07006><em>Spiking Neural Network Integrated Circuits: A Review of Trends and Future Directions</em></a>, Arindam Basu et al., 2022</a></li><li><a href=#reckon-a-28nm-sub-mm2-task-agnostic-spiking-recurrent-neural-network-processor-enabling-on-chip-learning-over-second-long-timescaleshttpsarxivorgabs220809759-charlotte-frenkel-and-giacomo-indiveri-2022><a href=https://arxiv.org/abs/2208.09759><em>ReckOn: A 28nm Sub-mm2 Task-Agnostic Spiking Recurrent Neural Network Processor Enabling On-Chip Learning over Second-Long Timescales</em></a>, Charlotte Frenkel and Giacomo Indiveri, 2022</a></li><li><a href=#sne-an-energy-proportional-digital-accelerator-for-sparse-event-based-convolutionshttpsarxivorgabs220410687-alfio-di-mauro-et-al-2022><a href=https://arxiv.org/abs/2204.10687><em>SNE: an Energy-Proportional Digital Accelerator for Sparse Event-Based Convolutions</em></a>, Alfio di Mauro et al., 2022</a></li><li><a href=#sparse-compressed-spiking-neural-network-accelerator-for-object-detectionhttpsarxivorgabs220500778-hong-han-lien-and-tian-shehuan-chang-2022><a href=https://arxiv.org/abs/2205.00778><em>Sparse Compressed Spiking Neural Network Accelerator for Object Detection</em></a>, Hong-Han Lien and Tian-Shehuan Chang, 2022.</a></li></ol></li><li><a href=#acknowledgements>Acknowledgements</a></li><li><a href=#credits>Credits</a></li><li><a href=#authors>Authors</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/digital-neuromophic-hardware-read-list/><img src=/p/digital-neuromophic-hardware-read-list/frenkel-thesis_hu87aec3c1b95cb5a45016ddc34478f587_199946_800x0_resize_box_3.png srcset="/p/digital-neuromophic-hardware-read-list/frenkel-thesis_hu87aec3c1b95cb5a45016ddc34478f587_199946_800x0_resize_box_3.png 800w, /p/digital-neuromophic-hardware-read-list/frenkel-thesis_hu87aec3c1b95cb5a45016ddc34478f587_199946_1600x0_resize_box_3.png 1600w" width=800 height=337 loading=lazy alt="Featured image of post Digital neuromophic hardware read list"></a></div><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/p/digital-neuromophic-hardware-read-list/>Digital neuromophic hardware read list</a></h2><h3 class=article-subtitle>List of research articles related to digital hardware for neuromorphic applications.</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg><time class=article-time--published>Jan 11, 2023</time></div><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>8 minute read</time></div></footer></div></header><section class=article-content><p>Here&rsquo;s a list of articles and theses related to digital hardware designs for neuomorphic applications. I plan to update it regularly. To be redirected directly to the sources, click on the titles!</p><p>If you are new to neuromorphic computing, I strongly suggest to get a grasp of how an SNN works from <a class=link href=https://arxiv.org/abs/2109.12894 target=_blank rel=noopener>this paper</a>. Otherwise, it will be pretty difficult to understand the content of the papers listed here.</p><h2 id=2015>2015</h2><h3 id=truenorth-design-and-tool-flow-of-a-65-mw-1-million-neuron-programmable-neurosynaptic-chiphttpsredwoodberkeleyeduwp-contentuploads202108akopyan2015pdf-filipp-akopyan-et-al-2015><a class=link href=https://redwood.berkeley.edu/wp-content/uploads/2021/08/Akopyan2015.pdf target=_blank rel=noopener><em>TrueNorth: Design and Tool Flow of a 65 mW 1 Million Neuron Programmable Neurosynaptic Chip</em></a>, Filipp Akopyan et al., 2015</h3><p>This a fully digital chip, embedding <strong>4096</strong> cores with <strong>1M</strong> neurons and <strong>256M</strong> synapses!</p><p>It adopts a mixed design methodology: the local computational cores are <strong>synchronous</strong>, while the interconnecting infrastructure is <strong>asynchronous</strong>, i.e. event-driven. In particular, each core adopts <strong>time-multiplexing</strong> to compute the states of its neurons minimizing core area; each core has <strong>256</strong> neurons associated.</p><p>TrueNorth claims to be operating in real-time: a <strong>1KHz synchronization signal</strong> is used to trigger the cores computations (state update, spikes processing, etc.). Moreover, they provide a software tool with one-to-one mapping to the hardware in order to deploy applications on it.</p><h2 id=2018>2018</h2><h3 id=loihi-a-neuromorphic-manycore-processor-with-on-chip-learninghttpsredwoodberkeleyeduwp-contentuploads202108davies2018pdf-mike-davies-et-al-2018><a class=link href=https://redwood.berkeley.edu/wp-content/uploads/2021/08/Davies2018.pdf target=_blank rel=noopener><em>Loihi: A Neuromorphic Manycore Processor with On-Chip Learning</em></a>, Mike Davies et al., 2018</h3><p>Probably the most popular neuromorphic processor right now. What distinguishes it from the other ones are the online learning capabilities coupled with a <strong>completely asynchronous</strong> design: cores and routing network are completely clock-less!</p><p>Loihi supports <strong>local and scalable learning rules</strong>, through spike traces corresponding to filtered pre-synaptic and post-synaptic spike trains with configurable time constants, multiple state variables per synapse in addition to the weight value, reward traces. Moreover, several computational primitives are provided: addition of stochastic noise to the neuron&rsquo;s synaptic current response; configurable and adaptable synaptic, axon and refractory delays; configurable dendtritic tree processing; neuron threshold adaptiation; scaling and saturation of synaptic weights.</p><p>The Loihi chip employs <strong>128 neuromorphic cores</strong>, each of which consisting of <strong>1024 primitive spiking neural units</strong>. Each Loihi core includes a <strong>programmable learning engine</strong>. Each core has an <strong>2Mb SRAM memory on-chip</strong>, with ECC overhead included. The chip is fabbed in Intel&rsquo;s <strong>14nm FinFET</strong> process.</p><h2 id=2019>2019</h2><h3 id=a-0086-mm2-127-pjsop-64k-synapse-256-neuron-online-learning-digital-spiking-neuromorphic-processor-in-28nm-cmoshttpsarxivorgabs180407858-charlotte-frenkel-et-al-2019><a class=link href=https://arxiv.org/abs/1804.07858 target=_blank rel=noopener><em>A 0.086-mm2 12.7-pJ/SOP 64k-Synapse 256-Neuron Online-Learning Digital Spiking Neuromorphic Processor in 28nm CMOS</em></a>, Charlotte Frenkel et al., 2019</h3><p>In this paper, a digital neuromorphic processor is presented. The Verilog is also <a class=link href=https://github.com/ChFrenkel/ODIN target=_blank rel=noopener>open source</a>!</p><p>The neurons states and the synapses weights are stored in two foundry SRAMs on chip. In order to emulate a crossbar, <strong>time-multiplexing</strong> is adopted: the synapses weights and neurons states are updated in a sequential manner instead of in parallel. On the core, <strong>256 neurons (4kB SRAM)</strong> and <strong>256x256 synapses (64kB SRAM)</strong> are embedded. This allows to get a very high synapses and neuron densities: <strong>741k synapses per squared millimiters</strong> and <strong>3k neurons per squared millimeters</strong>, using a <strong>28nm CMOS FDSOI</strong> process.</p><p>The neuron model is programmable through an SPI interface: the user can choose among a <strong>LIF</strong> model (<strong>8 bits</strong> for the state of each neuron) and <strong>Izhikevic</strong> one (<strong>55 bits</strong> for the state of each neuron). Online-learning capabilities are allowed with an hardware-efficient implementation of the <strong>Spike-Driven Synaptic Plasticity (SDSP)</strong> rule.</p><p>The design is fully <strong>synchronous</strong>. The time evolution of the SNN implemented on the core can be tuned choosing changing the frequency of the time reference events, allowing to update the neurons states only when events actually take place.
The result is that each Synaptic OPeration (SOP) requires only <strong>12.7pJ</strong> when the chip is powered with a voltage of 0.55V.</p><h3 id=morphic-a-65-nm-738k-synapsemm2-quad-core-binary-weight-digital-neuromorphic-processor-with-stochastic-spike-driven-online-learninghttpsarxivorgabs190408513-charlotte-frenkel-et-al-2019><a class=link href=https://arxiv.org/abs/1904.08513 target=_blank rel=noopener><em>MorphIC: A 65-nm 738k-Synapse/mm2 Quad-Core Binary-Weight Digital Neuromorphic Processor With Stochastic Spike-Driven Online Learning</em></a>, Charlotte Frenkel et al., 2019</h3><p>In this work, a <strong>quad-core neuromorphic processor</strong> is presented.</p><p>The neuron model employed is the <strong>LIF</strong> one. Synapses are quantized down to <strong>1 bit</strong> resolution, and online learning is allowed using a <strong>stochastic version of the SDSP rule</strong>. The chip is produced in <strong>65nm CMOS</strong>, embedding <strong>2k LIF neurons</strong> and <strong>2M synapses</strong>, reaching a density of <strong>738k synapses per squared millimeters</strong>.</p><p>The neurons interconnection is arranged in a hierarchical routing solution: <strong>mesh-based</strong> interconnectivity for <strong>out-of-chip</strong> communications; <strong>star-based</strong> connectivity for <strong>inter-core</strong> communications; <strong>crossbar-based</strong> interconnectivity for <strong>intra-core</strong> communications. 27 bits per neuron are allocated, allowing for a 1k neurons fan-in for each neuron, and 2k neurons fan-out for each neuron.</p><h2 id=2020>2020</h2><h3 id=always-on-sub-300-nw-event-driven-spiking-neural-network-based-on-spike-driven-clock-generation-and-clock--and-power-gating-for-an-ultra-low-power-intelligent-devicehttpsarxivorgabs200612314-dewei-wang-et-al-2020><a class=link href=https://arxiv.org/abs/2006.12314 target=_blank rel=noopener><em>Always-On, Sub-300-nW, Event-Driven Spiking Neural Network based on Spike-Driven Clock-Generation and Clock- and Power-Gating for an Ultra-Low-Power Intelligent Device</em></a>, Dewei Wang et al., 2020</h3><p>In this work, a <strong>synchronous</strong> architecture is proposed. The logic operates at <strong>Near Threshold Voltage</strong> (NTV), and <strong>clock gating</strong> and <strong>power gating</strong> are heavily used to minimize power consumption during idle operation, which results to be <strong>300nW</strong>. The chip is targeted at <strong>always-on applications</strong>, like keyword spotting (KWS); and it is prototyped on a <strong>65nm CMOS</strong> process. The design is an <strong>only-inference</strong> one, with no online-learning capabilities.</p><p>The architecture belongs to the <strong>feed-forward</strong> category: <strong>5 cores</strong> are used to implement fully connected spiking layers of <strong>Integrate and Fire</strong> (IF) neurons. To minimize power consumption, <strong>asynchronous wake-up circuits</strong> are employed to activate the layers only when there are <strong>incoming spikes</strong>.</p><p>On the <strong>GCSC</strong> and <strong>HeySnips</strong> datasets, the recognition accuracies are <strong>91.8%</strong> and <strong>95.8%</strong>, respectively. The total power consumption ranges between <strong>75nW</strong> and <strong>220nW</strong>.</p><h2 id=2021>2021</h2><h3 id=μbrain-an-event-driven-and-fully-synthesizable-architecture-for-spiking-neural-networkshttpswwwfrontiersinorgarticles103389fnins2021664208full-jan-stuijt-et-al-2021><a class=link href=https://www.frontiersin.org/articles/10.3389/fnins.2021.664208/full target=_blank rel=noopener><em>μBrain: An Event-Driven and Fully Synthesizable Architecture for Spiking Neural Networks</em></a>, Jan Stuijt et al., 2021</h3><p>This is an <strong>asynchronous digital architecture</strong>, with no online-learning capabilities provided. It is an inference-only chip.</p><p>The bit precision and network topology is chosen at <strong>synthesis</strong> time, while the neurons parameters and synapses weights can be programmed on chip. The neuron model employed is the <strong>Integrate and Fire (IF)</strong> one, with no leakage; the leakage can be added using an additional inhibitory input neuron to model it. A <strong>local clock</strong> is generated a neuron level when a spike arrives, so that the circuit consumes only static power when not operating. No time multiplexing is employed, the architecture is organised in a <strong>layer-by-layer</strong> fashion where all the neurons operate in parallel (i.e. each core corresponds to a neuron).</p><h2 id=2022>2022</h2><h3 id=spiking-neural-network-integrated-circuits-a-review-of-trends-and-future-directionshttpsarxivorgabs220307006-arindam-basu-et-al-2022><a class=link href=https://arxiv.org/abs/2203.07006 target=_blank rel=noopener><em>Spiking Neural Network Integrated Circuits: A Review of Trends and Future Directions</em></a>, Arindam Basu et al., 2022</h3><p>Nice <strong>survey</strong> paper that compares different ICs, both digital and mixed-signal ones.</p><h3 id=reckon-a-28nm-sub-mm2-task-agnostic-spiking-recurrent-neural-network-processor-enabling-on-chip-learning-over-second-long-timescaleshttpsarxivorgabs220809759-charlotte-frenkel-and-giacomo-indiveri-2022><a class=link href=https://arxiv.org/abs/2208.09759 target=_blank rel=noopener><em>ReckOn: A 28nm Sub-mm2 Task-Agnostic Spiking Recurrent Neural Network Processor Enabling On-Chip Learning over Second-Long Timescales</em></a>, Charlotte Frenkel and Giacomo Indiveri, 2022</h3><p>In this work, a <strong>Recurrent Spiking Neural Network (RSNN)</strong> processor is presented. The Verilog code is <a class=link href=https://github.com/chfrenkel/ReckON target=_blank rel=noopener>open source</a>.</p><p>The key feature of this chip is the online learning capability, using a modified version of the <strong>feed-forward eligibility traces</strong> algorithm, which is a bio-inspired approximation of the BackPropagation Through Time (BPTT) algorithm employed for artificial RNNs. The chip performance is validated on gesture recognition, keyword spotting and navigation, with <strong>sub-150μW</strong> and <strong>sub-squared millimeter</strong> power and area budgets.</p><h3 id=sne-an-energy-proportional-digital-accelerator-for-sparse-event-based-convolutionshttpsarxivorgabs220410687-alfio-di-mauro-et-al-2022><a class=link href=https://arxiv.org/abs/2204.10687 target=_blank rel=noopener><em>SNE: an Energy-Proportional Digital Accelerator for Sparse Event-Based Convolutions</em></a>, Alfio di Mauro et al., 2022</h3><p>In this work, an <strong>only-inference digital</strong> chip is presented. The design is tuned towards <strong>event cameras output processing</strong>, employing <strong>convolution engines</strong> in the hardware.</p><p>The novelty of this design is that, even if it is a <strong>synchronous one</strong>, the number of operations performed is proportional to the <strong>number of events</strong> recorded by the camera, which allows very efficient inference when dealing with <strong>sparse inputs</strong> (e.g. low activity scenarios).</p><p>The design is validated on the <strong>IBM DVSGesture</strong> dataset, obtaining <strong>80μJ per inference</strong> when classifying samples, with a recognition accuracy of <strong>92.80%</strong> at most. This design is also integrated in the <a class=link href=https://pulp-platform.org/ target=_blank rel=noopener>PULP</a> platform; the SoC that embeds it is described in <a class=link href=https://arxiv.org/abs/2209.01065 target=_blank rel=noopener><em>Kraken: A Direct Event/Frame-Based Multi-sensor Fusion SoC for Ultra-Efficient Visual Processing in Nano-UAVs</em></a>, Alfio Di Mauro et al., 2022.</p><p>The SystemVerilog code is <a class=link href=https://github.com/pulp-platform/sne target=_blank rel=noopener>open source</a>!</p><h3 id=sparse-compressed-spiking-neural-network-accelerator-for-object-detectionhttpsarxivorgabs220500778-hong-han-lien-and-tian-shehuan-chang-2022><a class=link href=https://arxiv.org/abs/2205.00778 target=_blank rel=noopener><em>Sparse Compressed Spiking Neural Network Accelerator for Object Detection</em></a>, Hong-Han Lien and Tian-Shehuan Chang, 2022.</h3><p>The neuron model employed in this work is a <strong>LIF</strong> one, with a delta-shaped synaptic kernel. The architecture topology is a <strong>feed-forward</strong> one, in which the neuron cores are arranged either is a cascade-fashion or in a configurable Processing Element (PE) array. The focus of this chip is to deal <strong>efficiently</strong> with the sparse nature of the activation maps in an SNN, by compressing the model with <strong>sparse data structures</strong> coupled with <strong>model pruning</strong> and <strong>8 bits fixed point</strong> parallelism to reduce the on-chip memory requirement. The SNN architecture is <strong>mixed with an ANN one</strong>.</p><p>The final implementation, validated on an <strong>object detection</strong> task, achieves <strong>29FPS</strong> when dealing with <strong>1024x576 input frames</strong>; the throughput efficiency is <strong>35.88TOPS/W</strong> and <strong>1.05mJ/frame</strong>, running at <strong>500MHz</strong> and being taped out on the <strong>TSMC 28nm CMOS</strong> process.</p><p>The object detection network is trained offline as ANN and then <strong>converted</strong> to an SNN, using the <strong>IVS 3 classes</strong> dataset and achieving <strong>71.5% maP</strong> with on-chip inference.</p><h2 id=acknowledgements>Acknowledgements</h2><p>I would like to thank <a class=link href=https://ch.frenkel.github.io target=_blank rel=noopener>Charlotte Frenkel</a> for the valuable comments and suggestions.</p><h2 id=credits>Credits</h2><p>The cover image is taken from <a class=link href=https://ch.frenkel.github.io target=_blank rel=noopener>Charlotte Frenkel</a>&rsquo;s thesis.</p><h2 id=authors>Authors</h2><ul><li><a class=link href=https://fabrizio-ottati.dev target=_blank rel=noopener>Fabrizio Ottati</a> is a Ph.D. student in the HLS Laboratory of the Department of Electronics and Communications, Politecnico di Torino. His main interests are event-based cameras, digital hardware design and neuromorphic computing. He is one of the maintainers of two open source projects in the field of neuromorphic computing, <a class=link href=https://tonic.readthedocs.io target=_blank rel=noopener>Tonic</a> and <a class=link href=https://expelliarmus.readthedocs.io target=_blank rel=noopener>Expelliarmus</a>, and one of the founders of <a class=link href=https://open-neuromorphic.org target=_blank rel=noopener>Open Neuromorphic</a>.</li></ul></section><footer class=article-footer><section class=article-tags><a href=/tags/research/>research</a>
<a href=/tags/hardware/>hardware</a>
<a href=/tags/digital/>digital</a>
<a href=/tags/neuromorphic/>neuromorphic</a>
<a href=/tags/snn/>snn</a>
<a href=/tags/ai/>AI</a></section></footer></article><footer class=site-footer><section class=copyright>&copy;
2022 -
2023 Open Neuromorphic</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.16.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>