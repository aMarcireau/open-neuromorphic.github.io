<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Events organised by ONM: talks, hands-on sessions and so on."><title>Events</title><link rel=canonical href=https://open-neuromorphic.org/events/><link rel=stylesheet href=/scss/style.min.8191399262444ab68b72a18c97392f5349be20a1615d77445be51e974c144cff.css><meta property="og:title" content="Events"><meta property="og:description" content="Events organised by ONM: talks, hands-on sessions and so on."><meta property="og:url" content="https://open-neuromorphic.org/events/"><meta property="og:site_name" content="Open Neuromorphic"><meta property="og:type" content="article"><meta property="article:section" content="Page"><meta property="og:image" content="https://open-neuromorphic.org/img/ONM.png"><meta name=twitter:title content="Events"><meta name=twitter:description content="Events organised by ONM: talks, hands-on sessions and so on."><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://open-neuromorphic.org/img/ONM.png"><link rel="shortcut icon" href=/img/ONM-logo.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/ONM-logo.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Open Neuromorphic</a></h1><h2 class=site-description>Organization that aims at providing one place to reference all relevant open-source project in the neuromorphic research domain.</h2></div></header><ol class=social-menu><li><a href=https://discord.gg/C9bzWgNmqk target=_blank title=Discord rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-discord" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#2c3e50" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="9" cy="12" r="1"/><circle cx="15" cy="12" r="1"/><path d="M7.5 7.5c3.5-1 5.5-1 9 0"/><path d="M7 16.5c3.5 1 6.5 1 10 0"/><path d="M15.5 17c0 1 1.5 3 2 3 1.5.0 2.833-1.667 3.5-3 .667-1.667.5-5.833-1.5-11.5-1.457-1.015-3-1.34-4.5-1.5l-1 2.5"/><path d="M8.5 17c0 1-1.356 3-1.832 3-1.429.0-2.698-1.667-3.333-3-.635-1.667-.476-5.833 1.428-11.5C6.151 4.485 7.545 4.16 9 4l1 2.5"/></svg></a></li><li><a href=https://github.com/open-neuromorphic target=_blank title=GitHub rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.linkedin.com/groups/9267873 target=_blank title=LinkedIn rel=me><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-linkedin" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#2c3e50" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="4" y="4" width="16" height="16" rx="2"/><line x1="8" y1="11" x2="8" y2="16"/><line x1="8" y1="8" x2="8" y2="8.01"/><line x1="12" y1="16" x2="12" y2="11"/><path d="M16 16v-3a2 2 0 00-4 0"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg><span>Home</span></a></li><li class=current><a href=/events/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-event" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><rect x="4" y="5" width="16" height="16" rx="2"/><line x1="16" y1="3" x2="16" y2="7"/><line x1="8" y1="3" x2="8" y2="7"/><line x1="4" y1="11" x2="20" y2="11"/><rect x="8" y="15" width="2" height="2"/></svg><span>Events</span></a></li><li><a href=/opportunities/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-bulb" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 12h1m8-9v1m8 8h1M5.6 5.6l.7.7m12.1-.7-.7.7"/><path d="M9 16a5 5 0 116 0 3.5 3.5.0 00-1 3 2 2 0 01-4 0 3.5 3.5.0 00-1-3"/><line x1="9.7" y1="17" x2="14.3" y2="17"/></svg><span>Opportunities</span></a></li><li><a href=/resources/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-code" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><polyline points="7 8 3 12 7 16"/><polyline points="17 8 21 12 17 16"/><line x1="14" y1="4" x2="10" y2="20"/></svg><span>Resources</span></a></li><li><a href=/team/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-users" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><circle cx="9" cy="7" r="4"/><path d="M3 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/><path d="M16 3.13a4 4 0 010 7.75"/><path d="M21 21v-2a4 4 0 00-3-3.85"/></svg><span>Team</span></a></li><li><a href=/about/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-brand-open-source" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M12 3a9 9 0 013.618 17.243l-2.193-5.602a3 3 0 10-2.849.0l-2.193 5.603A9 9 0 0112 3z"/></svg><span>About</span></a></li><li><a href=/getting-involved/><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-code" width="44" height="44" viewBox="0 0 24 24" stroke-width="1.5" stroke="#9e9e9e" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><polyline points="7 8 3 12 7 16"/><polyline points="17 8 21 12 17 16"/><line x1="14" y1="4" x2="10" y2="20"/></svg><span>Getting involved</span></a></li><div class=menu-bottom-section><li id=dark-mode-toggle><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg><span>Dark Mode</span></li></div></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#2022-12-13-first-edition-of-onm>2022-12-13: First Edition of ONM</a></li><li><a href=#2023-01-26-applied-brain-research>2023-01-26: Applied Brain Research</a><ol><li><a href=#event-description>Event description</a></li><li><a href=#speakers-bio>Speaker&rsquo;s bio</a></li><li><a href=#event-material>Event material</a></li></ol></li><li><a href=#2023-02-14-giorgia-dellaferrera>2023-02-14: Giorgia Dellaferrera</a><ol><li><a href=#event-description-1>Event description</a></li><li><a href=#speakers-bio-1>Speaker&rsquo;s bio</a></li><li><a href=#event-content>Event content</a></li></ol></li><li><a href=#2023-03-02-jason-eshraghian>2023-03-02: Jason Eshraghian</a><ol><li><a href=#event-description-2>Event description</a></li><li><a href=#speakers-bio-2>Speaker&rsquo;s bio</a></li></ol></li><li><a href=#2023-03-21-catherine-schuman>2023-03-21: Catherine Schuman</a><ol><li><a href=#event-description-3>Event description</a></li><li><a href=#speakers-bio-3>Speaker&rsquo;s bio</a></li></ol></li><li><a href=#2023-04-04-gregor-lenz>2023-04-04: Gregor Lenz</a><ol><li><a href=#event-description-4>Event description</a></li><li><a href=#speakers-bio-4>Speaker&rsquo;s bio</a></li></ol></li><li><a href=#2023-04-26-dylan-muir>2023-04-26: Dylan Muir</a><ol><li><a href=#event-description-5>Event description</a></li><li><a href=#speakers-bio-5>Speaker&rsquo;s bio</a></li></ol></li><li><a href=#2023-05-31-intel>2023-05-31: Intel</a><ol><li><a href=#event-description-6>Event description</a></li><li><a href=#abstract>Abstract</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/events/><img src=/img/ONM.png loading=lazy alt="Featured image of post Events"></a></div><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/events/>Events</a></h2><h3 class=article-subtitle>Events organised by ONM: talks, hands-on sessions and so on.</h3></div><footer class=article-time><div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg><time class=article-time--reading>5 minute read</time></div></footer></div></header><section class=article-content><h2 id=2022-12-13-first-edition-of-onm>2022-12-13: First Edition of ONM</h2><p>On the 13th of December 2022, the first edition of Open Neuromorphic took place! The invited speakers were:</p><ul><li><a class=link href=https://chfrenkel.github.io target=_blank rel=noopener>Charlotte Frenkel</a> (<a class=link href=/onm-events/event-22-12-13/slides/charlotte-frenkel.pdf>slides</a>).</li><li><a class=link href=https://lenzgregor.com target=_blank rel=noopener>Gregor Lenz</a> (<a class=link href=https://slides.com/gregorlenz/open-neuromorphic target=_blank rel=noopener>slides</a>).</li><li><a class=link href=https://jasoneshraghian.com target=_blank rel=noopener>Jason Eshraghian</a>.</li><li><a class=link href=https://services.ini.uzh.ch/people/melika target=_blank rel=noopener>Melika Payvand</a>.</li><li><a class=link href=https://jepedersen.dk target=_blank rel=noopener>Jens Egholm Pedersen</a> (<a class=link href=https://jepedersen.dk/slides/2212_ONM/index.html target=_blank rel=noopener>slides</a>).</li></ul><p>Host: <a class=link href=https://fabrizio-ottati.github.io target=_blank rel=noopener>Fabrizio Ottati</a> (<a class=link href=/onm-events/event-22-12-13/slides/fabrizio-ottati.pdf>slides</a>).</p><h2 id=2023-01-26-applied-brain-research>2023-01-26: Applied Brain Research</h2><p><img src=/events/trevor-bekolay.jpeg width=460 height=460 srcset="/events/trevor-bekolay_hufbec93062349485948350099f8024531_51540_480x0_resize_q75_box.jpeg 480w, /events/trevor-bekolay_hufbec93062349485948350099f8024531_51540_1024x0_resize_q75_box.jpeg 1024w" loading=lazy alt="Trevor Bekolay" class=gallery-image data-flex-grow=100 data-flex-basis=240px></p><h3 id=event-description>Event description</h3><ul><li><p><strong>Title:</strong> Nengo - Applied Brain Research.</p></li><li><p><strong>Time</strong>: 6PM - 7:30PM CET.</p></li></ul><h3 id=speakers-bio>Speaker&rsquo;s bio</h3><p>Trevor Bekolay’s primary research interest is in learning and memory. In his Master’s degree, he explored how to do supervised, unsupervised, and reinforcement learning in networks of biologically plausible spiking neurons. In his PhD, he applied this knowledge to the domain of speech to explore how sounds coming into the ear become high-level linguistic representations, and how those representations become sequences of vocal tract movements that produce speech.</p><p>Trevor is also passionate about reproducible science, particularly when complex software pipelines are involved. In 2013, he started a development effort to reimplement the Nengo neural simulator from scratch in Python, which has now grown to a project with over 20 contributors around the world.</p><h3 id=event-material>Event material</h3><ul><li><a class=link href=2023-01-26-Nengo.pdf>Slides</a>.</li><li><a class=link href=https://youtu.be/sgu9l_bqAHM target=_blank rel=noopener>Event recording</a>.</li></ul><h2 id=2023-02-14-giorgia-dellaferrera>2023-02-14: Giorgia Dellaferrera</h2><p><img src=/events/giorgia-dellaferrera.jpeg width=500 height=500 srcset="/events/giorgia-dellaferrera_hua839c980f4c66a1a65fd81ca426e703b_92518_480x0_resize_q75_box.jpeg 480w, /events/giorgia-dellaferrera_hua839c980f4c66a1a65fd81ca426e703b_92518_1024x0_resize_q75_box.jpeg 1024w" loading=lazy alt="Giorgia Dellaferrera" class=gallery-image data-flex-grow=100 data-flex-basis=240px></p><h3 id=event-description-1>Event description</h3><ul><li><p><strong>Title:</strong> PEPITA - A forward-forward alternative to backpropagation.</p></li><li><p><strong>Time:</strong> 6PM - 7:30PM CET.</p></li></ul><h3 id=speakers-bio-1>Speaker&rsquo;s bio</h3><p>Giorgia Dellaferrera has completed her PhD in computational neuroscience at the Institute of Neuroinformatics (ETH Zurich and the University of Zurich) and IBM Research Zurich with Prof. Indiveri, Prof. Eleftheriou and Dr. Pantazi. Her doctoral thesis focused on the interplay between neuroscience and artificial intelligence, with an emphasis on learning mechanisms in brains and machines. During her PhD, she visited the lab of Prof. Kreiman at the Harvard Medical School (US), where she developed a biologically inspired training strategy for artificial neural networks. Before her PhD, Giorgia obtained a master in Applied Physics at the Swiss Federal Institute of Technology Lausanne (EPFL) and worked as an intern at the Okinawa Institute of Science and Technology, Logitech, Imperial College London, and EPFL.</p><h3 id=event-content>Event content</h3><ul><li><a class=link href=https://forms.gle/umvmW3MXyV8AUXue8 target=_blank rel=noopener>Registration form</a>.</li></ul><h2 id=2023-03-02-jason-eshraghian>2023-03-02: Jason Eshraghian</h2><p><img src=/events/jason-eshraghian.webp width=602 height=480 srcset="/events/jason-eshraghian_hu5d2d5366ae47912bc22f41487225bf0b_38920_480x0_resize_q75_h2_box_2.webp 480w, /events/jason-eshraghian_hu5d2d5366ae47912bc22f41487225bf0b_38920_1024x0_resize_q75_h2_box_2.webp 1024w" loading=lazy alt="Jason Eshraghian" class=gallery-image data-flex-grow=125 data-flex-basis=301px></p><h3 id=event-description-2>Event description</h3><ul><li><p><strong>Title:</strong> Hands-on session with snnTorch.</p></li><li><p><strong>Time</strong>: 6PM - 7:30PM CET.</p></li></ul><h3 id=speakers-bio-2>Speaker&rsquo;s bio</h3><p><a class=link href=https://jasoneshraghian.com target=_blank rel=noopener>Jason K. Eshraghian</a> is an Assistant Professor at the Department of Electrical and Computer Engineering at UC Santa Cruz, CA, USA. Prior to that, he was a Post-Doctoral Researcher at the Department of Electrical Engineering and Computer Science, University of Michigan in Ann Arbor. He received the Bachelor of Engineering (Electrical and Electronic) and the Bachelor of Laws degrees from The University of Western Australia, WA, Australia in 2016, where he also completed his Ph.D. Degree.</p><p>Professor Eshraghian was awarded the 2019 IEEE VLSI Best Paper Award, the Best Paper Award at 2019 IEEE Artificial Intelligence CAS Conference, and the Best Live Demonstration Award at 2020 IEEE ICECS for his work on neuromorphic vision and in-memory computing using RRAM. He currently serves as the secretary-elect of the IEEE Neural Systems and Applications Committee, and was a recipient of the Fulbright Future Fellowship (Australian-America Fulbright Commission), the Forrest Research Fellowship (Forrest Research Foundation), and the Endeavour Fellowship (Australian Government).</p><h2 id=2023-03-21-catherine-schuman>2023-03-21: Catherine Schuman</h2><p><img src=/events/catherine-schuman.webp width=444 height=614 srcset="/events/catherine-schuman_huc056e41512c17dd339d4074ee71f8169_11002_480x0_resize_q75_h2_box_2.webp 480w, /events/catherine-schuman_huc056e41512c17dd339d4074ee71f8169_11002_1024x0_resize_q75_h2_box_2.webp 1024w" loading=lazy alt="Catherine Schuman" class=gallery-image data-flex-grow=72 data-flex-basis=173px></p><h3 id=event-description-3>Event description</h3><ul><li><p><strong>Title:</strong> Evolutionary Optimization for Neuromorphic Systems.</p></li><li><p><strong>Time:</strong> 6PM - 7:30PM CET.</p></li></ul><h3 id=speakers-bio-3>Speaker&rsquo;s bio</h3><p>Catherine (Katie) Schuman is an Assistant Professor in the Department of Electrical Engineering and Computer Science at the University of Tennessee (UT). She received her Ph.D. in Computer Science from UT in 2015, where she completed her dissertation on the use of evolutionary algorithms to train spiking neural networks for neuromorphic systems. Katie previously served as a research scientist at Oak Ridge National Laboratory, where her research focused on algorithms and applications of neuromorphic systems. Katie co-leads the TENNLab Neuromorphic Computing Research Group at UT. She has over 100 publications as well as seven patents in the field of neuromorphic computing. She received the Department of Energy Early Career Award in 2019.</p><h2 id=2023-04-04-gregor-lenz>2023-04-04: Gregor Lenz</h2><p><img src=/events/gregor-lenz.jpeg width=500 height=500 srcset="/events/gregor-lenz_hu401dcc7f29eb10f93ef8a57749c49e1b_50576_480x0_resize_q75_box.jpeg 480w, /events/gregor-lenz_hu401dcc7f29eb10f93ef8a57749c49e1b_50576_1024x0_resize_q75_box.jpeg 1024w" loading=lazy alt="Gregor Lenz" class=gallery-image data-flex-grow=100 data-flex-basis=240px></p><h3 id=event-description-4>Event description</h3><ul><li><p><strong>Abstract:</strong> Hands-on session with Sinabs and Speck.</p></li><li><p><strong>Time:</strong> 6PM - 7:30PM CET.</p></li></ul><h3 id=speakers-bio-4>Speaker&rsquo;s bio</h3><p><a class=link href=https://lenzgregor.com target=_blank rel=noopener>Gregor Lenz</a> graduated with a Ph.D. in neuromorphic engineering from Sorbonne University. He thinks that technology can learn a thing or two from how biological systems process information.</p><p>His main interests are event cameras that are inspired by the human retina and spiking neural networks that mimic human brain in an effort to teach machines to compute a bit more like humans do. At the very least there are some power efficiency gains to be made, but hopefully more! Also he loves to build open source software for spike-based machine learning. You can find more information on his personal website.</p><p>He is the maintainer of two open source projects in the field of neuromorphic computing, <a class=link href=https://tonic.readthedocs.io target=_blank rel=noopener>Tonic</a> and <a class=link href=https://expelliarmus.readthedocs.io target=_blank rel=noopener>expelliarmus</a>.</p><h2 id=2023-04-26-dylan-muir>2023-04-26: Dylan Muir</h2><p><img src=/events/dylan-muir.png width=400 height=400 srcset="/events/dylan-muir_hu1ad96f9805111af152bc8407747ed1dc_220801_480x0_resize_box_3.png 480w, /events/dylan-muir_hu1ad96f9805111af152bc8407747ed1dc_220801_1024x0_resize_box_3.png 1024w" loading=lazy alt="Dylan Muir" class=gallery-image data-flex-grow=100 data-flex-basis=240px></p><h3 id=event-description-5>Event description</h3><ul><li><p><strong>Title:</strong> Hands-on session with Xylo and Rockpool.</p></li><li><p><strong>Time:</strong> 6PM - 7:30PM CET.</p></li></ul><h3 id=speakers-bio-5>Speaker&rsquo;s bio</h3><p>Dylan Muir is the Vice President for Global Research Operations; Director for Algorithms and Applications; and Director for Global Business Development at SynSense. Dr. Muir is a specialist in architectures for neural computation. He has published extensively in computational and experimental neuroscience. At SynSense he is responsible for the company research vision, and directing development of neural architectures for signal processing. Dr. Muir holds a Doctor of Science (PhD) from ETH Zurich, and undergraduate degrees (Masters) in Electronic Engineering and in Computer Science from QUT, Australia.</p><h2 id=2023-05-31-intel>2023-05-31: Intel</h2><p><img src=/events/intel.png width=353 height=143 srcset="/events/intel_hu41a76559582f055efe3db8d65b118a4f_2802_480x0_resize_box_3.png 480w, /events/intel_hu41a76559582f055efe3db8d65b118a4f_2802_1024x0_resize_box_3.png 1024w" loading=lazy alt=Intel class=gallery-image data-flex-grow=246 data-flex-basis=592px></p><h3 id=event-description-6>Event description</h3><ul><li><p><strong>Title:</strong> Lava.</p></li><li><p><strong>Time:</strong> 6PM - 7:30PM CET.</p></li></ul><h3 id=abstract>Abstract</h3><p>Lava is an open-source software framework for developing neuro-inspired applications and mapping them to neuromorphic hardware. Lava provides developers with the tools and abstractions to develop applications that fully exploit the principles of neural computation. Constrained in this way, like the brain, Lava applications allow neuromorphic platforms to intelligently process, learn from, and respond to real-world data with great gains in energy efficiency and speed compared to conventional computer architectures.</p></section><footer class=article-footer></footer></article><footer class=site-footer><section class=copyright>&copy;
2022 -
2023 Open Neuromorphic</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.16.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>